#!/bin/bash
#SBATCH --partition=gpu
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=5
#SBATCH --gpus=1
#SBATCH --job-name=sleep
#SBATCH --ntasks=1
#SBATCH --time=8:30:00
#SBATCH --mem=64000M
#SBATCH --output=logs/slurm_output__tuner_%A.out

# run arbitrary command verbose
function runv {
  echo -e "\n\nrunning: $@"
  time "$@"
}

cd ~/thesis_app
. .venv/bin/activate
# poetry install

echo "ready to run"
pushd experiments/tuner

# now we can run arbitrary experiments
#### fine-tuning:
#export WANDB_MODE=offline 
# export EXP_NAME=full2
# runv tune run recipes/lora_finetune_single_device.py --config ./llama2_7B_qlora_single_device.yaml gradient_accumulation_steps=16

# export EXP_NAME=full3
# runv tune run recipes/lora_finetune_single_device.py --config ./llama2_7B_qlora_single_device.yaml gradient_accumulation_steps=32

# export EXP_NAME=full4
# runv tune run recipes/lora_finetune_single_device.py --config ./llama2_7B_qlora_single_device.yaml gradient_accumulation_steps=64

#### post fine-tuning: evaluation/benchmarking:
export JUDGE=gpt-4-0125-preview
export EXP_NAME=full4

echo "evaluating EXP_NAME=$EXP_NAME"
declare -a checkpoints=("0" "1" "2" "3")
for CHKP_NUM in "${checkpoints[@]}"; do
  runv tune run ./recipes/generate.py --config ./generation.yaml CHKP_NUM=$CHKP_NUM benchmark_fluency=true benchmark_judge=$JUDGE
done

echo "all done!"