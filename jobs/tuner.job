#!/bin/bash
#SBATCH --partition=gpu
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=5
#SBATCH --gpus=1
#SBATCH --job-name=tuner
#SBATCH --ntasks=1
#SBATCH --time=16:00:00
#SBATCH --mem=64000M
#SBATCH --output=logs/slurm_output__tuner_%A.out

# run arbitrary command verbose
function runv {
  echo -e "\n\nrunning: $@"
  time "$@"
}

cd ~/thesis_app
. .venv/bin/activate
# poetry install

echo "ready to run"
pushd experiments/tuner

# now we can run arbitrary experiments
#### fine-tuning:
#export WANDB_MODE=offline 

#######################
# export EXP_NAME=full1
# runv tune run recipes/lora_finetune_single_device.py --config ./llama2_7B_qlora_single_device.yaml gradient_accumulation_steps=8

# export EXP_NAME=full2
# runv tune run recipes/lora_finetune_single_device.py --config ./llama2_7B_qlora_single_device.yaml gradient_accumulation_steps=16

# export EXP_NAME=full3
# runv tune run recipes/lora_finetune_single_device.py --config ./llama2_7B_qlora_single_device.yaml gradient_accumulation_steps=32

# export EXP_NAME=full4
# runv tune run recipes/lora_finetune_single_device.py --config ./llama2_7B_qlora_single_device.yaml gradient_accumulation_steps=64
#######################
#
# TODO: these experiments were run wrong, should have been optimizer.lr=4e-4 and 5e-4 :(
# export EXP_NAME=lr4_8
# runv tune run recipes/lora_finetune_single_device.py --config ./llama2_7B_qlora_single_device.yaml epochs=3 gradient_accumulation_steps=8  lr=4e-4
# export EXP_NAME=lr5_8
# runv tune run recipes/lora_finetune_single_device.py --config ./llama2_7B_qlora_single_device.yaml epochs=3 gradient_accumulation_steps=8  lr=5e-4

# export EXP_NAME=lr4_16
# runv tune run recipes/lora_finetune_single_device.py --config ./llama2_7B_qlora_single_device.yaml epochs=3 gradient_accumulation_steps=16 lr=4e-4
# export EXP_NAME=lr5_16
# runv tune run recipes/lora_finetune_single_device.py --config ./llama2_7B_qlora_single_device.yaml epochs=3 gradient_accumulation_steps=16 lr=5e-4
#######################

#### post fine-tuning (evaluation/benchmarking):
echo "experiments done, skipping post processing"
exit 0
export JUDGE=gpt-4-0125-preview

declare -a exps=("lr4_8" "lr5_8" "lr4_16" "lr5_16")
for exp in "${exps[@]}"; do
  export EXP_NAME="$exp"
  echo "evaluating EXP_NAME=$EXP_NAME"
  declare -a checkpoints=("0" "1" "2" "3")
  for CHKP_NUM in "${checkpoints[@]}"; do
    runv tune run ./recipes/generate.py --config ./generation.yaml CHKP_NUM=$CHKP_NUM benchmark_fluency=true benchmark_judge=$JUDGE
  done
done

echo "all done!"